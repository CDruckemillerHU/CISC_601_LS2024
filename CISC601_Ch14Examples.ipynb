{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c70730-1ee2-4c4e-8e29-8bdc193d0065",
   "metadata": {},
   "source": [
    "Scientific Computing I\n",
    "Numerical Methods for Engineers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcce175-6a7f-4052-a464-99f8fc15853f",
   "metadata": {},
   "source": [
    "Chapter 14 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b1cabd-0d18-46f0-8336-47ea8fb5ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized max: 1.2494548919970994\n",
      "Actual max: 1.25\n"
     ]
    }
   ],
   "source": [
    "## Example 14.1\n",
    "## Use an RNG to locate the maximum of:\n",
    "## f(x,y) = y-x-2x^2-2xy-y^2\n",
    "## within the domain of -2 <= x <= 2 & 1 <= y <= 3\n",
    "\n",
    "import random\n",
    "\n",
    "def sample_function(x, y):\n",
    "    return y - x - (2 * x**2) - (2 * x * y) - (y**2)\n",
    "\n",
    "def randomizer(lower, upper):\n",
    "    return lower + (upper - lower) * random.random()\n",
    "\n",
    "def random_search_max(f, x_bounds, y_bounds, iterations=5000):\n",
    "    curr_max = float('-inf')  # Start with a very low value\n",
    "    for _ in range(iterations):\n",
    "        x = randomizer(*x_bounds)\n",
    "        y = randomizer(*y_bounds)\n",
    "        curr_max = max(f(x, y), curr_max)\n",
    "    return curr_max\n",
    "\n",
    "# Define search bounds and iterations\n",
    "x_bounds = (-2, 2)\n",
    "y_bounds = (1, 3)\n",
    "iterations = 5000\n",
    "\n",
    "# Perform random search\n",
    "random_max = random_search_max(sample_function, x_bounds, y_bounds, iterations)\n",
    "\n",
    "# Print results\n",
    "print(\"Randomized max:\", random_max)\n",
    "print(\"Actual max:\", sample_function(-1, 1.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ffcce-98a9-4b0a-93af-d906e5eee3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49635a03-4230-4acf-a7b6-ad7b1ea15ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached\n",
      "Maximum point: (7.4635, 10.3687)\n",
      "Maximum value: 802.3906\n"
     ]
    }
   ],
   "source": [
    "## Example 14.2\n",
    "## Employ the gradient to evaluate the steepest ascent direction for the function\n",
    "## f(x,y) = xy^2 at the point (2,2). Assume positive X points east and positive y points north\n",
    "\n",
    "import math\n",
    "\n",
    "# Function definition\n",
    "def sample_function(x, y):\n",
    "    return x * y**2\n",
    "\n",
    "# Partial derivatives\n",
    "def df_dx(x, y):\n",
    "    return y**2\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return 2 * x * y\n",
    "\n",
    "# Gradient ascent function\n",
    "def gradient_ascent(f, grad_x, grad_y, initial_point, step_size=0.1, tol=1e-6, max_iter=100):\n",
    "    x, y = initial_point\n",
    "    for i in range(max_iter):\n",
    "        grad = (grad_x(x, y), grad_y(x, y))  # Calculate gradient\n",
    "        grad_magnitude = math.sqrt(grad[0]**2 + grad[1]**2)\n",
    "        \n",
    "        # Normalize gradient direction\n",
    "        unit_grad = (grad[0] / grad_magnitude, grad[1] / grad_magnitude)\n",
    "        \n",
    "        # Update point\n",
    "        x_new = x + step_size * unit_grad[0]\n",
    "        y_new = y + step_size * unit_grad[1]\n",
    "        \n",
    "        # Check for convergence\n",
    "        if math.sqrt((x_new - x)**2 + (y_new - y)**2) < tol:\n",
    "            print(f\"Converged after {i+1} iterations\")\n",
    "            return x_new, y_new, f(x_new, y_new)\n",
    "        \n",
    "        x, y = x_new, y_new\n",
    "    \n",
    "    print(\"Maximum iterations reached\")\n",
    "    return x, y, f(x, y)\n",
    "\n",
    "# Initial point and parameters\n",
    "initial_point = (2, 2)\n",
    "step_size = 0.1\n",
    "\n",
    "# Perform gradient ascent\n",
    "result = gradient_ascent(\n",
    "    sample_function, df_dx, df_dy, initial_point, step_size=step_size\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"Maximum point: ({result[0]:.4f}, {result[1]:.4f})\")\n",
    "print(f\"Maximum value: {result[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a31322b-c212-4b12-8b1f-3d7219a179d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum of g(h): 0.2000 at h = 0.2000\n"
     ]
    }
   ],
   "source": [
    "## Example 14.3\n",
    "## Using the 2-D function:\n",
    "## f(x,y) = 2xy + 2x-x^2-2y^2\n",
    "## Develop a 1D version of this equation along the gradient direction at point x=-1 y=1\n",
    "\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Original 2D function\n",
    "def f(x, y):\n",
    "    return 2 * x * y + 2 * x - x**2 - 2 * y**2\n",
    "\n",
    "# 1D function along the gradient direction\n",
    "def g(h):\n",
    "    return -180 * h**2 + 72 * h - 7\n",
    "\n",
    "# Gradient ascent direction at (-1, 1)\n",
    "grad_x = 6  # df/dx = 2y + 2 - 2x\n",
    "grad_y = -6  # df/dy = 2x - 4y\n",
    "\n",
    "# Generate points for visualization\n",
    "h_values = np.linspace(-1, 1, 500)\n",
    "g_values = g(h_values)\n",
    "\n",
    "# Plot the 1D function\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(h_values, g_values, label=r\"$g(h) = -180h^2 + 72h - 7$\")\n",
    "# plt.axhline(0, color=\"black\", linewidth=0.8, linestyle=\"--\")\n",
    "# plt.axvline(0, color=\"black\", linewidth=0.8, linestyle=\"--\")\n",
    "# plt.title(\"1D Function Along Gradient Direction\")\n",
    "# plt.xlabel(\"h (distance along gradient direction)\")\n",
    "# plt.ylabel(\"g(h)\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# Find the maximum value analytically or numerically\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "result = minimize_scalar(lambda h: -g(h), bounds=(-1, 1), method=\"bounded\")\n",
    "max_h = result.x\n",
    "max_g = g(max_h)\n",
    "\n",
    "print(f\"Maximum of g(h): {max_g:.4f} at h = {max_h:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69dbf1d7-23f8-4114-a77f-5cb65a82a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 78 iterations.\n",
      "Maximum value: 2.0000 at x = 2.0000, y = 1.0000\n"
     ]
    }
   ],
   "source": [
    "## Example 14.4\n",
    "## Max the following function:\n",
    "## f(x,y) = 2xy + 2x - x^2 -2y^2\n",
    "## Using initial guessses of x = -1 and y = 1\n",
    "## Analyitical solution (follow example to get to f(2,1) = max.\n",
    "\n",
    "def f(x, y):\n",
    "    return 2 * x * y + 2 * x - x**2 - 2 * y**2\n",
    "\n",
    "def partial_x(x, y):\n",
    "    return 2 * y + 2 - 2 * x\n",
    "\n",
    "def partial_y(x, y):\n",
    "    return 2 * x - 4 * y\n",
    "\n",
    "def gradient_ascent_2d(f, grad_x, grad_y, x0, y0, learning_rate=0.1, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Perform gradient ascent to maximize a 2D function.\n",
    "\n",
    "    :param f: Function to maximize\n",
    "    :param grad_x: Partial derivative with respect to x\n",
    "    :param grad_y: Partial derivative with respect to y\n",
    "    :param x0: Initial x-coordinate\n",
    "    :param y0: Initial y-coordinate\n",
    "    :param learning_rate: Step size for updates\n",
    "    :param tol: Convergence tolerance\n",
    "    :param max_iter: Maximum number of iterations\n",
    "    :return: Maximum value and coordinates\n",
    "    \"\"\"\n",
    "    x, y = x0, y0\n",
    "    for i in range(max_iter):\n",
    "        # Calculate gradients\n",
    "        grad_i = grad_x(x, y)\n",
    "        grad_j = grad_y(x, y)\n",
    "        \n",
    "        # Update x and y\n",
    "        x_new = x + learning_rate * grad_i\n",
    "        y_new = y + learning_rate * grad_j\n",
    "        \n",
    "        # Check for convergence\n",
    "        if abs(x_new - x) < tol and abs(y_new - y) < tol:\n",
    "            print(f\"Converged after {i + 1} iterations.\")\n",
    "            break\n",
    "        \n",
    "        # Update variables\n",
    "        x, y = x_new, y_new\n",
    "    \n",
    "    return f(x, y), x, y\n",
    "\n",
    "# Initial guesses\n",
    "x0, y0 = -1, 1\n",
    "\n",
    "# Perform gradient ascent\n",
    "max_value, max_x, max_y = gradient_ascent_2d(\n",
    "    f, partial_x, partial_y, x0, y0, learning_rate=0.2\n",
    ")\n",
    "\n",
    "print(f\"Maximum value: {max_value:.4f} at x = {max_x:.4f}, y = {max_y:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450967d-2378-42e5-a6ae-fb467e08b9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
